# LLM Prompts for Textbook Q&A Extractor
# Edit these prompts to customize extraction behavior
# Variables use {variable_name} syntax and are filled in at runtime

identify_chapters:
  description: "Identify chapter boundaries from PDF page index"
  prompt: |
    Analyze this textbook page index and identify all chapters that contain QUESTIONS sections.

    This is a medical textbook where each chapter has:
    - A chapter header (e.g., "Chapter 1: Title" or "1 Title")
    - A QUESTIONS section with numbered questions
    - An ANSWERS section with explanations

    For each chapter with questions, provide:
    - chapter_number: The chapter number (integer)
    - title: The chapter title
    - start_page: The page number where the chapter starts (from the [PAGE X] markers)
    - has_questions: true (only include chapters that have questions)

    Look for patterns like "QUESTIONS" or numbered questions (1., 2., etc.) to identify which chapters have Q&A content.

    Return ONLY a JSON array, no other text:
    [
      {{"chapter_number": 1, "title": "Chapter Title", "start_page": 14, "has_questions": true}},
      ...
    ]

    PAGE INDEX:
    {page_index}

generate_cloze_cards:
  description: "Generate cloze deletion flashcards from explanation text"
  prompt: |
    You are creating Anki cloze deletion flashcards from a medical textbook explanation.

    EXPLANATION TEXT (from {question_id}):
    {explanation}

    TASK: Extract ALL distinct facts and concepts as cloze deletion cards.

    ============================================================
    CORE PRINCIPLE: MINIMUM INFORMATION
    ============================================================
    Each card tests ONE atomic fact. If a sentence contains multiple learnable facts,
    create MULTIPLE cards. The source material is extremely high-density - err on the
    side of creating MORE cards rather than fewer. A single explanation may yield
    1-2 cards (if brief) or 20+ cards (if detailed). Let the content dictate the count.

    Card quality checklist:
    - Does this card test exactly ONE fact? If not, split it.
    - Is the sentence as SHORT as possible while retaining meaning?
    - Does the context constrain the answer to one reasonable response?
    - Would a student need to truly recall this, not just recognize it?

    ============================================================
    CLOZE SYNTAX RULES (NOTES vs CARDS)
    ============================================================
    IMPORTANT: Understand the difference between NOTES and CARDS in Anki:
    - A NOTE is one item in your JSON output (one cloze_text)
    - A note with {{{{c1::X}}}} and {{{{c2::Y}}}} generates TWO CARDS automatically
    - Card 1 hides X (shows Y), Card 2 hides Y (shows X)

    RULE: If multiple testable facts come from the SAME sentence or tightly related
    concept, put them on ONE note using c1, c2, c3. Do NOT create separate notes.

    BAD - Three separate notes for related facts:
    Note 1: "<b>{{{{c1::Mazabraud syndrome::syndrome}}}}</b> combines intramuscular myxomas and fibrous dysplasia."
    Note 2: "Mazabraud syndrome combines <b>{{{{c1::intramuscular myxomas::condition}}}}</b> and fibrous dysplasia."
    Note 3: "Mazabraud syndrome combines intramuscular myxomas and <b>{{{{c1::fibrous dysplasia::condition}}}}</b>."

    GOOD - One note with three clozes (generates 3 cards):
    "<b>{{{{c1::Mazabraud syndrome::syndrome}}}}</b> combines <b>{{{{c2::intramuscular myxomas::condition}}}}</b> and <b>{{{{c3::fibrous dysplasia::condition}}}}</b>."

    SYNTAX RULES:
    1. Use {{{{c1::text::hint}}}} format - ALWAYS include a semantic hint
    2. BOLD all cloze deletions using <b> tags
    3. BOLD important non-clozed terms (diagnoses, structures, key findings) for visual emphasis
    4. Use c1, c2, c3 on SAME note for facts from the same sentence/concept
    5. Create SEPARATE notes only for unrelated facts or different sentences
    6. Keep sentences SHORT - front-load the topic/category for context

    CRITICAL - PREVENTING ANSWER INFERENCE:
    Review each card for ways a user could INFER the answer without true recall.

    PROBLEM 1: COMPLEMENTARY/OPPOSITE PAIRS
    When two clozes are opposites or complements, use the SAME cloze number so both are
    hidden together. Otherwise the user can infer one answer from the other.

    BAD: "In ependymoma, {{{{c1::younger::age}}}} patients → posterior fossa, {{{{c2::older::age}}}} → supratentorial."
    (If user sees "younger" hidden but "older" shown, they can guess the answer)

    GOOD: "In ependymoma, {{{{c1::younger::age}}}} patients → posterior fossa, {{{{c1::older::age}}}} → supratentorial."
    (Both hidden together, must recall complete relationship)

    Apply to: younger/older, pediatric/adult, increased/decreased, higher/lower, proximal/distal,
    anterior/posterior, medial/lateral, superior/inferior, early/late, acute/chronic, larger/smaller, etc.

    PROBLEM 2: SEQUENTIAL/ORDINAL VALUES
    When clozes contain values from an obvious sequence, seeing one reveals the other.

    BAD: "Classic ependymomas are WHO grade {{{{c1::II::grade number}}}}, while anaplastic ependymomas are WHO grade {{{{c2::III::grade number}}}}."
    (User sees "II" and knows the other must be "III" from the WHO grade sequence I-IV)

    GOOD: "Classic ependymomas are WHO grade {{{{c1::II::grade number}}}}, while anaplastic ependymomas are WHO grade {{{{c1::III::grade number}}}}."
    (Both hidden together - user must know BOTH grades)

    Apply to: WHO grades (I/II/III/IV), TNM stages, numbered classifications, Roman numerals, etc.

    PROBLEM 3: SEMANTICALLY RELATED TERMS
    When non-clozed text contains terms anatomically or conceptually tied to the clozed answer.

    BAD: "Central neurocytomas are predominantly located in the {{{{c1::lateral ventricle::location}}}} attached to the {{{{c2::septum pellucidum::structure}}}} or ventricular wall."
    (The terms "septum pellucidum" and "ventricular wall" are strongly associated with "lateral ventricle" - a knowledgeable user can infer c1 from the surrounding anatomy)

    GOOD - Option A (cloze both related terms):
    "Central neurocytomas are predominantly located in the {{{{c1::lateral ventricle::location}}}} attached to the {{{{c1::septum pellucidum::structure}}}} or ventricular wall."
    (Hiding both removes the contextual hint)

    GOOD - Option B (restructure to avoid leakage):
    "<b>Central neurocytomas</b> attach to the {{{{c1::septum pellucidum::structure}}}} within the lateral ventricle."
    (Single fact, no inference possible)

    When creating cards, ask: "Could a user guess this answer from OTHER information in the sentence?"
    If yes, either (a) use the same cloze number for related terms, or (b) restructure the card.

    BRACE SYNTAX - COUNT CAREFULLY:
    The cloze format is: <b>{{{{c1::answer::hint}}}}</b>

    Step-by-step construction:
    1. Start with <b>
    2. Add exactly TWO opening braces: {{{{
    3. Add cN:: (e.g., c1::, c2::)
    4. Add your answer text
    5. Add :: and hint
    6. Add exactly TWO closing braces: }}}}
    7. End with </b>

    Verify each cloze: <b> + {{{{ + cN:: + text + :: + hint + }}}} + </b>

    HINT CATEGORIES (use these or similar):
    - Anatomical: structure, location, region, pathway, vessel, nerve, muscle, bone
    - Clinical: finding, symptom, sign, presentation, complication
    - Diagnostic: technique, modality, imaging finding, test, criteria
    - Pathology: condition, disease, mechanism, etiology, cause
    - Quantitative: percentage, number, measurement, duration, age, frequency
    - Comparative: difference, feature, characteristic, distinguishing feature

    ============================================================
    WHAT TO EXTRACT (create a card for each)
    ============================================================
    - Every specific term, name, or diagnosis mentioned
    - Every number, percentage, or statistic
    - Every anatomical relationship (X is located in Y, X innervates Y)
    - Every causal relationship (X causes Y, X leads to Y)
    - Every distinguishing feature in differentials
    - Every imaging finding or diagnostic criterion
    - Every mechanism or pathway step
    - Every clinical presentation or symptom association
    - Every treatment or management point

    ============================================================
    GOOD EXAMPLES (with proper note structure and bolding)
    ============================================================
    SINGLE-CLOZE NOTES (for isolated facts):
    - "<b>Brainstem gliomas</b> account for <b>{{{{c1::10-20%::percentage}}}}</b> of pediatric intracranial tumors."
    - "<b>{{{{c1::DNET::diagnosis}}}}</b> most commonly occurs in the <b>temporal lobe</b>."
    - "<b>Radiography</b> is <b>{{{{c1::insensitive::limitation}}}}</b> for detecting early <b>bone loss</b>."

    MULTI-CLOZE NOTES (for related facts from same concept - generates multiple cards):
    - "The <b>{{{{c1::foramen spinosum::structure}}}}</b> transmits the <b>{{{{c2::middle meningeal artery::vessel}}}}</b>."
      (One note → 2 cards: one tests structure, one tests vessel)

    - "<b>{{{{c1::DNET::diagnosis}}}}</b> presents with <b>{{{{c2::intractable seizures::symptom}}}}</b> in the <b>{{{{c3::temporal lobe::location}}}}</b>."
      (One note → 3 cards)

    - "<b>{{{{c1::Ganglioglioma::diagnosis}}}}</b> differs from <b>DNET</b> by more frequent <b>{{{{c2::calcification::finding}}}}</b> and <b>{{{{c3::contrast enhancement::finding}}}}</b>."
      (One note → 3 cards, with DNET bolded but not clozed for context)

    - "<b>{{{{c1::Intraosseous lipomas::diagnosis}}}}</b> are <b>{{{{c2::benign::behavior}}}}</b> fatty tumors of <b>bone</b>."
      (One note → 2 cards)

    Notice: Bold ALL key medical terms for visual scanning, whether clozed or not.
    When facts are from the same sentence, combine into one note with c1, c2, c3.

    ============================================================
    CRITICAL: STANDALONE CONTEXT
    ============================================================
    Cards will be reviewed randomly mixed with cards from OTHER topics and chapters.
    The NON-clozed text must provide enough context to identify the domain/topic.

    Ask yourself: "If a student sees this card mixed with 1000 other cards from
    different subjects, can they tell what field/topic this is about?"

    BAD - No context to constrain the answer:
    - "<b>{{{{c1::Intraosseous lipomas::diagnosis}}}}</b> are benign lesions."
      Problem: "[diagnosis] are benign lesions" - hundreds of things are benign!
      The student has no idea this is about bone tumors.

    GOOD - Context anchors the topic:
    - "<b>{{{{c1::Intraosseous lipomas::diagnosis}}}}</b> are benign fatty bone tumors."
    - "In bone radiology, <b>{{{{c1::intraosseous lipomas::diagnosis}}}}</b> are benign lesions."

    BAD - Generic context:
    - "<b>{{{{c1::Chordoma::diagnosis}}}}</b> is a malignant tumor."
      Problem: Malignant tumor of what? Brain? Bone? Soft tissue?

    GOOD - Specific context:
    - "<b>{{{{c1::Chordoma::diagnosis}}}}</b> is a malignant tumor arising from notochord remnants."
    - "Sacral/clival tumors: <b>{{{{c1::Chordoma::diagnosis}}}}</b> arises from notochord remnants."

    The rule: Include at least ONE specific anchor (organ system, location, imaging
    modality, patient population, or distinguishing feature) in the non-clozed text.

    ============================================================
    BAD EXAMPLES (avoid these patterns)
    ============================================================
    - "The answer is {{{{c1::C}}}}." (trivial, no educational value)
    - "{{{{c1::This}}}} is a common finding." (orphan cloze, no context)
    - "The {{{{c1::patient}}}} had symptoms." (generic, not specific)
    - "{{{{c1::Radiographs}}}} detect bone loss." (missing hint and bold)
    - "The {{{{c1::foramen spinosum::structure}}}} transmits the {{{{c2::middle meningeal artery::vessel}}}}." (not bolded)
    - Long sentence with buried cloze (keep it short, front-load context)
    - "<b>{{{{c1::X::diagnosis}}}}</b> is benign/malignant." (no organ system or context)

    ============================================================
    GUIDELINES
    ============================================================
    - Create as MANY cards as needed to capture ALL distinct facts
    - Prefer multiple simple cards over fewer complex cards
    - Each card should stand alone without the original question
    - Skip only: citations, references, question-specific phrasing ("In this case...")
    - When in doubt, make a card - more coverage is better

    Return ONLY a JSON array (no other text):
    [
      {{
        "cloze_text": "The <b>{{{{c1::foramen spinosum::structure}}}}</b> transmits the <b>{{{{c2::middle meningeal artery::vessel}}}}</b>.",
        "learning_point": "Foramen spinosum anatomy",
        "confidence": "high",
        "category": "anatomy"
      }},
      {{
        "cloze_text": "<b>{{{{c1::Mazabraud syndrome::syndrome}}}}</b> combines <b>{{{{c2::intramuscular myxomas::condition}}}}</b> and <b>{{{{c3::fibrous dysplasia::condition}}}}</b>.",
        "learning_point": "Mazabraud syndrome components",
        "confidence": "high",
        "category": "pathology"
      }},
      {{
        "cloze_text": "<b>Brainstem gliomas</b> account for <b>{{{{c1::10-20%::percentage}}}}</b> of pediatric <b>intracranial tumors</b>.",
        "learning_point": "Brainstem glioma epidemiology",
        "confidence": "high",
        "category": "statistics"
      }}
    ]

    Categories: anatomy, pathology, imaging, clinical, differential, statistics, mechanism

    If no suitable content for cloze cards, return: []

identify_question_blocks:
  description: "Identify question BLOCKS (grouped by main question number) with line ranges"
  prompt: |
    Identify ALL question blocks in this chapter, starting from block 1.

    TEXT FORMAT:
    - Each line starts with [LINE:NNNN] marker
    - Page breaks appear as [PAGE N] - content may span multiple pages
    - Questions section comes first, then answers section

    WHAT IS A BLOCK?
    A block groups related content by main question number:
    - Block "1" = all content for question 1 (context + 1a + 1b + 1c, etc.)
    - Block "2" = all content for question 2 (context + 2a + 2b, etc.)
    - Standalone questions (no sub-parts) are their own block

    QUESTION SECTION (what to capture in question_start to question_end):
    - Clinical context/scenario (e.g., "A 55-year-old male presents with...")
    - All sub-questions for this block (1a, 1b, 1c, etc.)
    - ALL answer choices (A, B, C, D) for each sub-question
    - IMPORTANT: Choices may appear on lines AFTER the question text, possibly across page breaks
    - question_end should be the LAST line containing choices for this block, just before the next block's context begins

    ANSWER SECTION (what to capture in answer_start to answer_end):
    The answer section contains TWO types of content - capture ALL of it:
    1. SPECIFIC ANSWERS: Lines like "1a Answer B." or "Answer 1a. B." with brief explanations
    2. SHARED DISCUSSION: Additional educational content that follows, including:
       - "Imaging Findings:" sections
       - "Discussion:" sections
       - "Differential Diagnosis:" sections
       - "References:" sections
       - Any other explanatory text before the next block's answers begin

    HOW TO FIND BOUNDARIES:
    - question_start: First line of this block's CONTENT. This varies by textbook format:
      * Format A: "1  A 55-year-old presents..." (number + context on same line)
      * Format B: Question text on line N, then question number alone on line N+1

      CRITICAL FOR FORMAT B - Example:
      [LINE:004944] What is the diagnosis?    ← question_start should be 4944
      [LINE:004945] 1                          ← This is just the question number
      [LINE:004946] A.  Option A

      When a line contains ONLY a question number (like "1" or "2a"), the actual question content
      starts earlier. Read backwards to find where the question text or context begins - this may
      be one line or several lines before the number. Use your understanding of the content to
      identify the true start of each question block.

    - question_end: Last line of choices before the NEXT block's content begins
    - answer_start: First line of answers for this block (e.g., "1a  Answer B." or "Answer 1a.")
    - answer_end: Last line before the next block's answer section begins (include all discussion content)

    CRITICAL: Start from block 1. Do not skip any blocks. Scan from the beginning of the text.

    OUTPUT FORMAT - one line per block, pipe-delimited with ONLY integers:
    block_id|question_start|question_end|answer_start|answer_end

    IMPORTANT: Output ONLY the line NUMBERS (integers), not the line content.
    Extract the number from [LINE:NNNN] markers - output just NNNN as an integer.

    Example: If block 1 starts at [LINE:0005] and ends at [LINE:0015], with answers from [LINE:0288] to [LINE:0321]:
    1|5|15|288|321

    Use 0 for answer_start and answer_end if no answer section exists for a block.
    Output ONLY the pipe-delimited lines with integers, nothing else.

    CHAPTER TEXT:
    {chapter_text}

format_raw_block:
  description: "Format a raw question/answer block into structured JSON"
  prompt: |
    Parse this raw question block into structured JSON.

    BLOCK ID: {block_id}

    RAW QUESTION TEXT:
    {question_text}

    RAW ANSWER TEXT:
    {answer_text}

    PARSING RULES:

    1. MARKERS TO UNDERSTAND:
       - [LINE:NNNN] markers: Ignore these, they are line number metadata
       - [PAGE N] markers: These indicate page breaks - extract page numbers for questions/answers

    2. FIND THE QUESTION IDENTIFIERS in the text:
       - Look for numbers like "1", "2", "3" OR sub-question IDs like "1a", "1b", "2a", "2b"
       - These may appear on their OWN LINE, separate from the question text
       - Example: "...decreased susceptibility\n1\nartifact?" means question ID is "1"
       - Example: "What is the diagnosis?\n2a\nA. Option..." means question ID is "2a"

    3. DETERMINE BLOCK TYPE:
       - STANDALONE: Block has only one question (e.g., just "1" or "3")
       - MULTI-PART: Block has sub-questions (e.g., "2a", "2b" under context "2")

    4. EXTRACT CONTEXT (for multi-part blocks):
       - Context is ONLY shared introductory text that applies to ALL sub-questions
       - Example WITH context: "A patient presents for arthrogram. [scenario details]..." followed by "2a What is the diagnosis?" and "2b What is the treatment?"
         → context.text = "A patient presents for arthrogram. [scenario details]..."
         → 2a.question_text = "What is the diagnosis?"
         → 2b.question_text = "What is the treatment?"
       - Example WITHOUT context: "What is the diagnosis? 2a [choices] What is the treatment? 2b [choices]"
         → context.text = "" (empty - each sub-question has its own distinct question)
         → 2a.question_text = "What is the diagnosis?"
         → 2b.question_text = "What is the treatment?"
       - IMPORTANT: Do NOT put the first sub-question's text in context.text. Only truly shared scenario text goes in context.

    5. FOR EACH QUESTION/SUB-QUESTION, extract:
       - local_id: The ACTUAL ID from the text (e.g., "1", "2a", "3", "15b")
       - question_text: The question WITHOUT choices
       - choices: The A/B/C/D options
       - correct_answer: The letter (find in answer text: "Answer A" or "2a Answer B")
       - explanation: See EXPLANATION STRUCTURE below

    6. EXPLANATION STRUCTURE (IMPORTANT):
       The answer section often has TWO parts:
       A. SPECIFIC ANSWERS: Each sub-question has its own answer line (e.g., "2a Answer C. [explanation]", "2b Answer A. [explanation]")
       B. SHARED DISCUSSION: Content that applies to all sub-questions (Imaging Findings, Discussion, Differential Diagnosis, References)

       For each sub-question's `explanation` field, include:
       1. That sub-question's SPECIFIC answer text VERBATIM (e.g., for 2a: "Answer C. MRI of brain... [full text]")
       2. The SHARED discussion content (Imaging Findings, Discussion, etc.) - this goes to ALL sub-questions
       3. Do NOT include other sub-questions' specific answers (e.g., 2a should NOT contain "2b Answer A...")

       Example for block with 2a and 2b:
       - Raw answer text: "2a Answer C. MRI explanation... 2b Answer A. Glioblastoma explanation... [Imaging Findings]... [Discussion]..."
       - 2a.explanation = "Answer C. MRI explanation... [Imaging Findings]... [Discussion]..." (2a's answer + shared, NO 2b answer)
       - 2b.explanation = "Answer A. Glioblastoma explanation... [Imaging Findings]... [Discussion]..." (2b's answer + shared, NO 2a answer)

       For STANDALONE questions (no sub-parts), include the entire answer section verbatim.

    7. IMAGE DISTRIBUTION ([IMAGE: filename.jpeg] markers):
       Images appear in TWO sections - track them separately:

       A. QUESTION IMAGES (from RAW QUESTION TEXT):
          - CONTEXT IMAGES: Before the first sub-question (in scenario/case description)
          - SUB-QUESTION IMAGES: Within or immediately after a specific sub-question

          Assign to each sub-question's `image_files` array:
          - Each sub-question gets: ALL context images + that sub-question's specific images
          - Example: context has [img1.jpg], sub-question 2b has [img2.jpg]:
            - 2a.image_files = ["img1.jpg"]  (context only)
            - 2b.image_files = ["img1.jpg", "img2.jpg"]  (context + specific)

       B. ANSWER IMAGES (from RAW ANSWER TEXT):
          - Images in the answer/explanation section
          - These should ONLY appear in explanations, not with the question

          Assign to each sub-question's `answer_image_files` array:
          - If an answer image relates to a specific sub-question, add it there
          - If it's in shared_discussion, add to `shared_discussion.image_files`

       When in doubt:
       - Question section images → treat as context (all sub-questions)
       - Answer section images → put in shared_discussion.image_files

    8. SHARED DISCUSSION (from answer section):
       - Imaging Findings, Discussion, Differential Diagnosis, References
       - This is captured BOTH in shared_discussion AND included in each sub-question's explanation
       - The shared_discussion field is for reference; the explanation field is what gets displayed

    9. PAGE NUMBERS (from [PAGE N] markers):
       - Look for [PAGE N] markers in the text to determine page numbers
       - question_pages: List of page numbers where the question appears (from RAW QUESTION TEXT)
       - answer_pages: List of page numbers where the answer appears (from RAW ANSWER TEXT)
       - If a sub-question spans multiple pages, include all page numbers
       - If no [PAGE N] marker is found, use an empty array

    OUTPUT FORMAT - Return ONLY valid JSON:
    {{
      "block_id": "{block_id}",
      "context": {{
        "text": "Shared scenario text (empty string if standalone question)",
        "image_files": ["context_image.jpeg"]
      }},
      "sub_questions": [
        {{
          "local_id": "2a",
          "question_text": "Question text only, no choices",
          "choices": {{"A": "Option A", "B": "Option B", "C": "Option C", "D": "Option D"}},
          "correct_answer": "B",
          "explanation": "Answer B. [2a's specific answer verbatim]... [Imaging Findings verbatim]... [Discussion verbatim]... (includes shared content, excludes other sub-question answers)",
          "image_files": ["question_image.jpeg"],
          "answer_image_files": ["answer_image.jpeg"],
          "question_pages": [14, 15],
          "answer_pages": [42]
        }}
      ],
      "shared_discussion": {{
        "imaging_findings": "Imaging findings text if present",
        "discussion": "Discussion text if present",
        "differential_diagnosis": "Differential diagnosis text if present",
        "references": ["Reference 1", "Reference 2"],
        "full_text": "All additional educational content",
        "image_files": ["shared_answer_image.jpeg"]
      }}
    }}

    CRITICAL:
    - Use the ACTUAL question ID from the text, not just the block_id
    - For standalone questions, sub_questions has ONE entry with local_id = the question number
    - Copy all text VERBATIM (except for line markers) - do NOT summarize or paraphrase
    - Include the COMPLETE explanation text from the answer section, not a summary
    - AVOID REDUNDANT REPETITION: If content appears identically in both question and answer sections, do not repeat it verbatim in the explanation. However, when including any content, always reproduce it VERBATIM - never summarize or paraphrase

generate_cloze_cards_from_block:
  description: "Generate cloze cards from an entire question block with full context"
  prompt: |
    You are creating Anki cloze deletion flashcards from a medical textbook question block.

    This is a COMPLETE block containing all related content - use ALL of it.

    BLOCK ID: {block_id}
    CHAPTER: {chapter_num}

    NOTE: The text below may be RAW and UNFORMATTED - extracted directly from PDF.
    It may contain:
    - [IMAGE: filename.jpg] markers indicating where images appear
    - Question numbers (e.g., "1", "2a", "7b") mixed into the text
    - Multiple choice options (A. B. C. D.)
    - References at the end of answers
    - Minor formatting irregularities

    This is INTENTIONAL - you are a powerful model that can parse raw text.
    Focus on extracting the EDUCATIONAL CONTENT, not the formatting.

    ============================================================
    QUESTION SECTION (clinical scenario, question stem, choices):
    ============================================================
    {context_text}

    ============================================================
    ANSWER SECTION (correct answer, explanation, discussion):
    ============================================================
    {shared_discussion_text}

    {sub_questions_text}

    ============================================================
    TASK: Create cloze cards from ALL educational content
    ============================================================

    Parse the raw text above to extract:
    1. The clinical scenario and key findings
    2. The correct answer and WHY it is correct
    3. Explanatory content about the condition, anatomy, pathology, etc.
    4. Differential diagnosis points if mentioned
    5. Key facts from references if educationally valuable

    IGNORE: Question numbers, image markers, formatting artifacts, and the choices themselves
    (don't create cards about "A is correct" - create cards about the CONCEPT)

    CLOZE SYNTAX:
    - Use {{{{c1::text::hint}}}} format with semantic hints
    - BOLD all clozes using <b> tags
    - BOLD important terms for visual emphasis
    - Use c1, c2, c3 on same card for related facts from same sentence
    - Create separate cards for unrelated facts

    CRITICAL - PREVENTING ANSWER INFERENCE:
    Review each card for ways a user could INFER the answer without true recall.

    PROBLEM 1: COMPLEMENTARY/OPPOSITE PAIRS
    When two clozes are opposites or complements, use the SAME cloze number so both are
    hidden together. Otherwise the user can infer one answer from the other.

    BAD: "{{{{c1::younger::age}}}} patients → posterior fossa, {{{{c2::older::age}}}} → supratentorial"
    (User sees "older" and can guess "younger")

    GOOD: "{{{{c1::younger::age}}}} patients → posterior fossa, {{{{c1::older::age}}}} → supratentorial"
    (Both hidden together, must recall complete relationship)

    Apply to: younger/older, increased/decreased, proximal/distal, anterior/posterior, early/late, etc.

    PROBLEM 2: SEQUENTIAL/ORDINAL VALUES
    When clozes contain values from an obvious sequence, seeing one reveals the other.

    BAD: "Classic ependymomas are WHO grade {{{{c1::II::grade number}}}}, while anaplastic ependymomas are WHO grade {{{{c2::III::grade number}}}}."
    (User sees "II" and knows the other must be "III" from the WHO grade sequence I-IV)

    GOOD: "Classic ependymomas are WHO grade {{{{c1::II::grade number}}}}, while anaplastic ependymomas are WHO grade {{{{c1::III::grade number}}}}."
    (Both hidden together - user must know BOTH grades)

    Apply to: WHO grades (I/II/III/IV), TNM stages, numbered classifications, Roman numerals, etc.

    PROBLEM 3: SEMANTICALLY RELATED TERMS
    When non-clozed text contains terms anatomically or conceptually tied to the clozed answer.

    BAD: "Central neurocytomas are predominantly located in the {{{{c1::lateral ventricle::location}}}} attached to the {{{{c2::septum pellucidum::structure}}}} or ventricular wall."
    (The terms "septum pellucidum" and "ventricular wall" are strongly associated with "lateral ventricle" - a knowledgeable user can infer c1 from the surrounding anatomy)

    GOOD - Option A (cloze both related terms):
    "Central neurocytomas are predominantly located in the {{{{c1::lateral ventricle::location}}}} attached to the {{{{c1::septum pellucidum::structure}}}} or ventricular wall."
    (Hiding both removes the contextual hint)

    GOOD - Option B (restructure to avoid leakage):
    "<b>Central neurocytomas</b> attach to the {{{{c1::septum pellucidum::structure}}}} within the lateral ventricle."
    (Single fact, no inference possible)

    When creating cards, ask: "Could a user guess this answer from OTHER information in the sentence?"
    If yes, either (a) use the same cloze number for related terms, or (b) restructure the card.

    BRACE SYNTAX - COUNT CAREFULLY:
    The cloze format is: <b>{{{{c1::answer::hint}}}}</b>

    Step-by-step construction:
    1. Start with <b>
    2. Add exactly TWO opening braces: {{{{
    3. Add cN:: (e.g., c1::, c2::)
    4. Add your answer text
    5. Add :: and hint
    6. Add exactly TWO closing braces: }}}}
    7. End with </b>

    Verify each cloze: <b> + {{{{ + cN:: + text + :: + hint + }}}} + </b>

    Return ONLY a JSON array:
    [
      {{
        "cloze_text": "<b>{{{{c1::Finding::type}}}}</b> is seen in this <b>condition</b>.",
        "learning_point": "Key concept being tested",
        "confidence": "high",
        "category": "imaging"
      }},
      ...
    ]

    Categories: anatomy, pathology, imaging, clinical, differential, statistics, mechanism

    If no suitable content, return: []
