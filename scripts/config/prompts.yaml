# LLM Prompts for Textbook Q&A Extractor
# Edit these prompts to customize extraction behavior
# Variables use {variable_name} syntax and are filled in at runtime

identify_chapters:
  description: "Identify chapter boundaries from PDF page index"
  prompt: |
    Analyze this textbook page index and identify all chapters that contain QUESTIONS sections.

    This is a medical textbook where each chapter has:
    - A chapter header (e.g., "Chapter 1: Title" or "1 Title")
    - A QUESTIONS section with numbered questions
    - An ANSWERS section with explanations

    For each chapter with questions, provide:
    - chapter_number: The chapter number (integer)
    - title: The chapter title
    - start_page: The page number where the chapter starts (from the [PAGE X] markers)
    - has_questions: true (only include chapters that have questions)

    Look for patterns like "QUESTIONS" or numbered questions (1., 2., etc.) to identify which chapters have Q&A content.

    Return ONLY a JSON array, no other text:
    [
      {{"chapter_number": 1, "title": "Chapter Title", "start_page": 14, "has_questions": true}},
      ...
    ]

    PAGE INDEX:
    {page_index}

extract_qa_pairs:
  description: "Extract Q&A pairs from a single chapter"
  prompt: |
    You are analyzing Chapter {chapter_num} of a medical textbook to extract all questions and their corresponding answers.

    TASK:
    1. Find all questions in the QUESTIONS section
    2. Find all answers in the ANSWERS section
    3. Match each question to its answer
    4. Identify the correct answer choice (A, B, C, D, or E)
    5. Identify shared context relationships between questions

    CRITICAL - VERBATIM EXTRACTION:
    You MUST copy all text EXACTLY as it appears in the source. Do NOT paraphrase, summarize, or shorten:
    - Question text: Copy the COMPLETE question text word-for-word
    - Answer choices: Copy each choice (A, B, C, D, E) EXACTLY as written
    - Explanations: Copy the COMPLETE explanation/answer text VERBATIM from the ANSWERS section

    Do NOT truncate or abbreviate any text. Include every word from the original.

    SHARED CONTEXT DETECTION:
    Some questions share context - a clinical scenario, case description, or introductory text that
    applies to multiple follow-up questions. Identify these relationships:

    - PATTERN 1: Explicit multi-part (e.g., "1" provides context, "1a", "1b", "1c" are questions)
      The context entry has no answer choices. Sub-questions reference it.

    - PATTERN 2: Implicit shared context (e.g., "Questions 5-7 refer to the following case...")
      Multiple standalone questions share the same introductory scenario.

    - PATTERN 3: Sequential related questions (e.g., Q2 says "In the same patient..." referring to Q1)
      Later questions explicitly reference earlier ones.

    For each question, set context_source:
    - null if the question is standalone (no shared context)
    - The question ID that provides context (e.g., "1" for question "1a")

    ADDITIONAL RULES:
    - Questions may have sub-parts like 2a, 2b, 2c - treat each as a separate question
    - Question IDs should match exactly as they appear (e.g., "1", "1a", "1b", "2a", "2b", "3")
    - For images: If a question references an image (e.g., "image below", "figure", "radiograph shown"), mark has_image: true
    - Use image_group to indicate which questions share the same image (e.g., "1" for questions 1, 1a, 1b, 1c sharing one image)

    Return ONLY a JSON object in this exact format:
    {{
      "chapter": {chapter_num},
      "questions": [
        {{
          "id": "1",
          "text": "Complete clinical scenario/context text copied verbatim",
          "choices": {{}},
          "has_image": true,
          "image_group": "1",
          "correct_answer": "",
          "explanation": "",
          "context_source": null
        }},
        {{
          "id": "1a",
          "text": "Complete question text copied verbatim",
          "choices": {{
            "A": "Complete choice A text verbatim",
            "B": "Complete choice B text verbatim",
            "C": "Complete choice C text verbatim",
            "D": "Complete choice D text verbatim"
          }},
          "has_image": false,
          "image_group": "1",
          "correct_answer": "B",
          "explanation": "Complete explanation copied verbatim from the ANSWERS section",
          "context_source": "1"
        }},
        {{
          "id": "2",
          "text": "Standalone question text copied verbatim",
          "choices": {{
            "A": "Choice A",
            "B": "Choice B",
            "C": "Choice C",
            "D": "Choice D"
          }},
          "has_image": false,
          "image_group": null,
          "correct_answer": "A",
          "explanation": "Explanation copied verbatim",
          "context_source": null
        }}
      ]
    }}

    CHAPTER TEXT:
    {chapter_text}

match_images_to_questions:
  description: "Match images to questions based on flanking text context"
  prompt: |
    Match images to questions for Chapter {chapter_num}.

    TEXTBOOK STRUCTURE (critical for correct matching):
    In this textbook, each question appears in this order:
    1. Question text ending with the question number (e.g., "...needle placement? 2a")
    2. IMAGE(S) - one OR MORE images may appear here
    3. Answer choices (A, B, C, D)
    4. Next question...

    Therefore: The question number at the END of "Text BEFORE image" tells you which question the image belongs to.

    QUESTIONS:
    {questions_text}

    IMAGES (with surrounding text context):
    {images_text}

    MATCHING RULES:
    1. Find the LAST question number mentioned in "Text BEFORE image" - that's the question this image belongs to
    2. Example: If text before ends with "...rotator interval approach? 2a" -> image belongs to ch{chapter_num}_2a
    3. Example: If text before ends with "...mixture for the arthrogram? 2b" -> image belongs to ch{chapter_num}_2b
    4. The text AFTER typically shows the answer choices, then the NEXT question
    5. MULTIPLE IMAGES CAN BELONG TO THE SAME QUESTION - if two or more images have the same question number in their "Text BEFORE", assign ALL of them to that question
    6. Example: If two images both have text before ending with "...image of the foot? 40" -> BOTH images belong to ch{chapter_num}_40
    7. Do NOT share a single image across multiple different questions
    8. Decorative images or images not matching any question -> assign to "(none)"

    Return ONLY a JSON object mapping image filenames to question IDs:
    {{
      "image1.jpeg": "ch{chapter_num}_40",
      "image2.jpeg": "ch{chapter_num}_40",
      "another_image.jpeg": "(none)"
    }}

postprocess_questions:
  description: "Link context questions to their sub-questions"
  prompt: |
    Analyze this list of extracted questions and identify context relationships.

    TASK:
    1. Identify "context-only" entries - these have descriptive text but NO answer choices (empty choices dict)
    2. For each context-only entry, find its related sub-questions by matching the image_group
    3. Return the updated questions with context properly linked

    RULES:
    - A question is "context-only" if it has no choices (choices is empty {{}}) AND its ID is just a number (e.g., "5") not a letter suffix (e.g., "5a")
    - Sub-questions share the same image_group as their context question
    - Example: If question "5" has image_group="5" and no choices, and questions "5a", "5b", "5c" also have image_group="5", then 5a/5b/5c are sub-questions of context "5"

    FOR EACH QUESTION, add these fields:
    - "is_context_only": true/false - true if this is just context (no choices, no correct answer)
    - "context": "" - for sub-questions, copy the context question's text here. Leave empty for standalone questions.
    - "context_question_id": "" - for sub-questions, the local_id of their context question (e.g., "5"). Leave empty otherwise.

    QUESTIONS TO PROCESS:
    {questions_json}

    Return ONLY the updated JSON array with all original fields preserved plus the new fields added.

associate_context:
  description: "Identify context relationships and merge context into sub-questions"
  prompt: |
    Analyze these questions from a medical textbook and identify CONTEXT relationships.

    WHAT IS A CONTEXT RELATIONSHIP?
    Sometimes a clinical scenario, case description, or introductory paragraph provides context
    that multiple follow-up questions refer to. The context entry itself is NOT a standalone
    question - it provides background information that the actual questions build upon.

    AVAILABLE DATA FOR EACH QUESTION:
    - full_id: Complete question identifier (e.g., "ch1_5")
    - local_id: Question number within chapter (e.g., "5", "5a")
    - text_preview: First 300 characters of the question text
    - has_choices: Whether the question has A/B/C/D answer options
    - num_choices: Number of answer choices (0-5)
    - has_correct_answer: Whether a correct answer was identified
    - has_explanation: Whether an explanation/answer text exists

    ABSOLUTE RULE - NEVER VIOLATE THIS:
    A question with has_choices: true can NEVER be a context_id.
    Context entries MUST have has_choices: false and num_choices: 0.
    If a question has answer choices (A/B/C/D), it is a REAL QUESTION, not context.

    COMMON MISTAKE TO AVOID:
    Questions like "2a" and "2b" are often BOTH real questions that happen to share a clinical
    scenario in their text. Do NOT assume "2a" is context for "2b" just because of the naming.
    Look at has_choices - if BOTH have choices, NEITHER is context. They are independent questions.

    VALID CONTEXT PATTERN:
    - Question "5" has has_choices: false, num_choices: 0 (just a scenario, no A/B/C/D)
    - Questions "5a", "5b", "5c" have has_choices: true (actual questions with choices)
    - In this case, "5" IS context for "5a", "5b", "5c"

    INVALID - DO NOT CREATE:
    - Question "2a" has has_choices: true, num_choices: 4
    - Question "2b" has has_choices: true, num_choices: 4
    - Both are real questions. Do NOT mark "2a" as context for "2b".

    QUESTIONS:
    {questions_summary}

    Return a JSON object:
    {{
      "context_mappings": [
        {{
          "context_id": "ch1_5",
          "sub_question_ids": ["ch1_5a", "ch1_5b", "ch1_5c"]
        }}
      ]
    }}

    If there are no context relationships, return: {{"context_mappings": []}}

    Return ONLY the JSON, no other text.

extract_line_ranges:
  description: "First pass: identify line ranges for each Q&A pair (minimal output)"
  prompt: |
    Analyze this chapter text and identify all question-answer pairs.

    The text has line numbers in format [LINE:NNNN]. Images are marked as [IMAGE: filename].

    TEXTBOOK STRUCTURE:
    - QUESTIONS section: Contains numbered questions with answer choices (A/B/C/D/E)
    - ANSWERS section: Contains explanations starting with "Answer X." where X is the correct letter
    - Questions and answers are in the same order (Question 1 matches Answer 1, etc.)

    CRITICAL - IMAGES AND CHOICES:
    - Images appear BETWEEN the question text and the answer choices
    - The typical structure is: Question text → [IMAGE] → A. choice → B. choice → C. choice → D. choice
    - You MUST include the answer choices (A/B/C/D) in question_end, even when an image appears before them
    - question_end should be the line number of the LAST choice (D or E), NOT the line before the image

    SHARED CONTEXT DETECTION:
    Some questions share context - a clinical scenario, case description, or introductory text that
    applies to multiple follow-up questions. Identify these relationships:

    - PATTERN 1: Explicit multi-part (e.g., "1" provides context, "1a", "1b", "1c" are questions)
      The context entry has no answer choices. Sub-questions reference it.

    - PATTERN 2: Implicit shared context (e.g., "Questions 5-7 refer to the following case...")
      Multiple standalone questions share the same introductory scenario.

    - PATTERN 3: Sequential related questions (e.g., Q2 says "In the same patient..." referring to Q1)
      Later questions explicitly reference earlier ones.

    For each question, determine:
    - context_source: The question_id that provides context for THIS question, or null if standalone
    - Example: "1a" might have context_source: "1" if question "1" provides the scenario
    - Example: "6" might have context_source: "5" if Q5 has shared context for Q5-7

    For EACH question, output:
    - question_id: The question number as it appears (e.g., "1", "1a", "2b", "15")
    - question_start: Line number where question text begins
    - question_end: Line number of the LAST answer choice (D or E line), or last line of question text if no choices
    - answer_start: Line number where answer/explanation begins (0 if no answer)
    - answer_end: Line number where answer ends (0 if no answer)
    - correct_letter: The correct answer letter (A/B/C/D/E), or "" for context-only entries
    - image_files: Array of image filenames that appear within this question's line range
    - context_source: The question_id providing context for this question, or null if standalone

    Return ONLY a JSON array:
    [
      {{
        "question_id": "1",
        "question_start": 5,
        "question_end": 8,
        "answer_start": 0,
        "answer_end": 0,
        "correct_letter": "",
        "image_files": ["p014_y0321_x0172_0.jpeg"],
        "context_source": null
      }},
      {{
        "question_id": "1a",
        "question_start": 9,
        "question_end": 14,
        "answer_start": 450,
        "answer_end": 460,
        "correct_letter": "B",
        "image_files": [],
        "context_source": "1"
      }},
      {{
        "question_id": "2",
        "question_start": 15,
        "question_end": 22,
        "answer_start": 461,
        "answer_end": 475,
        "correct_letter": "A",
        "image_files": [],
        "context_source": null
      }},
      ...
    ]

    CHAPTER TEXT:
    {chapter_text}

format_qa_pair:
  description: "Second pass: format a single Q&A pair into structured JSON"
  prompt: |
    Extract and format this question-answer pair from a medical textbook.

    QUESTION ID: {question_id}

    QUESTION TEXT (copy verbatim):
    {question_text}

    ANSWER TEXT (copy verbatim):
    {answer_text}

    INSTRUCTIONS:
    1. Copy ALL text EXACTLY as it appears - do not paraphrase or shorten
    2. Extract the question text (before the choices)
    3. Extract each answer choice (A, B, C, D, and E if present)
    4. Copy the full explanation verbatim
    5. If this is a context-only question (no choices), leave choices empty and mark is_context_only: true

    Return ONLY a JSON object:
    {{
      "id": "{question_id}",
      "text": "Full question text copied verbatim",
      "choices": {{
        "A": "Choice A text verbatim",
        "B": "Choice B text verbatim",
        "C": "Choice C text verbatim",
        "D": "Choice D text verbatim"
      }},
      "has_image": true,
      "image_group": "1",
      "correct_answer": "A",
      "explanation": "Full explanation copied verbatim",
      "is_context_only": false
    }}

    For context-only questions (no choices):
    {{
      "id": "{question_id}",
      "text": "Clinical scenario text copied verbatim",
      "choices": {{}},
      "has_image": true,
      "image_group": "1",
      "correct_answer": "",
      "explanation": "",
      "is_context_only": true
    }}

generate_cloze_cards:
  description: "Generate cloze deletion flashcards from explanation text"
  prompt: |
    You are creating Anki cloze deletion flashcards from a medical textbook explanation.

    EXPLANATION TEXT (from {question_id}):
    {explanation}

    TASK: Extract ALL distinct facts and concepts as cloze deletion cards.

    ============================================================
    CORE PRINCIPLE: MINIMUM INFORMATION
    ============================================================
    Each card tests ONE atomic fact. If a sentence contains multiple learnable facts,
    create MULTIPLE cards. The source material is extremely high-density - err on the
    side of creating MORE cards rather than fewer. A single explanation may yield
    1-2 cards (if brief) or 20+ cards (if detailed). Let the content dictate the count.

    Card quality checklist:
    - Does this card test exactly ONE fact? If not, split it.
    - Is the sentence as SHORT as possible while retaining meaning?
    - Does the context constrain the answer to one reasonable response?
    - Would a student need to truly recall this, not just recognize it?

    ============================================================
    CLOZE SYNTAX RULES (NOTES vs CARDS)
    ============================================================
    IMPORTANT: Understand the difference between NOTES and CARDS in Anki:
    - A NOTE is one item in your JSON output (one cloze_text)
    - A note with {{{{c1::X}}}} and {{{{c2::Y}}}} generates TWO CARDS automatically
    - Card 1 hides X (shows Y), Card 2 hides Y (shows X)

    RULE: If multiple testable facts come from the SAME sentence or tightly related
    concept, put them on ONE note using c1, c2, c3. Do NOT create separate notes.

    BAD - Three separate notes for related facts:
    Note 1: "<b>{{{{c1::Mazabraud syndrome::syndrome}}}}</b> combines intramuscular myxomas and fibrous dysplasia."
    Note 2: "Mazabraud syndrome combines <b>{{{{c1::intramuscular myxomas::condition}}}}</b> and fibrous dysplasia."
    Note 3: "Mazabraud syndrome combines intramuscular myxomas and <b>{{{{c1::fibrous dysplasia::condition}}}}</b>."

    GOOD - One note with three clozes (generates 3 cards):
    "<b>{{{{c1::Mazabraud syndrome::syndrome}}}}</b> combines <b>{{{{c2::intramuscular myxomas::condition}}}}</b> and <b>{{{{c3::fibrous dysplasia::condition}}}}</b>."

    SYNTAX RULES:
    1. Use {{{{c1::text::hint}}}} format - ALWAYS include a semantic hint
    2. BOLD all cloze deletions using <b> tags
    3. BOLD important non-clozed terms (diagnoses, structures, key findings) for visual emphasis
    4. Use c1, c2, c3 on SAME note for facts from the same sentence/concept
    5. Create SEPARATE notes only for unrelated facts or different sentences
    6. Keep sentences SHORT - front-load the topic/category for context

    HINT CATEGORIES (use these or similar):
    - Anatomical: structure, location, region, pathway, vessel, nerve, muscle, bone
    - Clinical: finding, symptom, sign, presentation, complication
    - Diagnostic: technique, modality, imaging finding, test, criteria
    - Pathology: condition, disease, mechanism, etiology, cause
    - Quantitative: percentage, number, measurement, duration, age, frequency
    - Comparative: difference, feature, characteristic, distinguishing feature

    ============================================================
    WHAT TO EXTRACT (create a card for each)
    ============================================================
    - Every specific term, name, or diagnosis mentioned
    - Every number, percentage, or statistic
    - Every anatomical relationship (X is located in Y, X innervates Y)
    - Every causal relationship (X causes Y, X leads to Y)
    - Every distinguishing feature in differentials
    - Every imaging finding or diagnostic criterion
    - Every mechanism or pathway step
    - Every clinical presentation or symptom association
    - Every treatment or management point

    ============================================================
    GOOD EXAMPLES (with proper note structure and bolding)
    ============================================================
    SINGLE-CLOZE NOTES (for isolated facts):
    - "<b>Brainstem gliomas</b> account for <b>{{{{c1::10-20%::percentage}}}}</b> of pediatric intracranial tumors."
    - "<b>{{{{c1::DNET::diagnosis}}}}</b> most commonly occurs in the <b>temporal lobe</b>."
    - "<b>Radiography</b> is <b>{{{{c1::insensitive::limitation}}}}</b> for detecting early <b>bone loss</b>."

    MULTI-CLOZE NOTES (for related facts from same concept - generates multiple cards):
    - "The <b>{{{{c1::foramen spinosum::structure}}}}</b> transmits the <b>{{{{c2::middle meningeal artery::vessel}}}}</b>."
      (One note → 2 cards: one tests structure, one tests vessel)

    - "<b>{{{{c1::DNET::diagnosis}}}}</b> presents with <b>{{{{c2::intractable seizures::symptom}}}}</b> in the <b>{{{{c3::temporal lobe::location}}}}</b>."
      (One note → 3 cards)

    - "<b>{{{{c1::Ganglioglioma::diagnosis}}}}</b> differs from <b>DNET</b> by more frequent <b>{{{{c2::calcification::finding}}}}</b> and <b>{{{{c3::contrast enhancement::finding}}}}</b>."
      (One note → 3 cards, with DNET bolded but not clozed for context)

    - "<b>{{{{c1::Intraosseous lipomas::diagnosis}}}}</b> are <b>{{{{c2::benign::behavior}}}}</b> fatty tumors of <b>bone</b>."
      (One note → 2 cards)

    Notice: Bold ALL key medical terms for visual scanning, whether clozed or not.
    When facts are from the same sentence, combine into one note with c1, c2, c3.

    ============================================================
    CRITICAL: STANDALONE CONTEXT
    ============================================================
    Cards will be reviewed randomly mixed with cards from OTHER topics and chapters.
    The NON-clozed text must provide enough context to identify the domain/topic.

    Ask yourself: "If a student sees this card mixed with 1000 other cards from
    different subjects, can they tell what field/topic this is about?"

    BAD - No context to constrain the answer:
    - "<b>{{{{c1::Intraosseous lipomas::diagnosis}}}}</b> are benign lesions."
      Problem: "[diagnosis] are benign lesions" - hundreds of things are benign!
      The student has no idea this is about bone tumors.

    GOOD - Context anchors the topic:
    - "<b>{{{{c1::Intraosseous lipomas::diagnosis}}}}</b> are benign fatty bone tumors."
    - "In bone radiology, <b>{{{{c1::intraosseous lipomas::diagnosis}}}}</b> are benign lesions."

    BAD - Generic context:
    - "<b>{{{{c1::Chordoma::diagnosis}}}}</b> is a malignant tumor."
      Problem: Malignant tumor of what? Brain? Bone? Soft tissue?

    GOOD - Specific context:
    - "<b>{{{{c1::Chordoma::diagnosis}}}}</b> is a malignant tumor arising from notochord remnants."
    - "Sacral/clival tumors: <b>{{{{c1::Chordoma::diagnosis}}}}</b> arises from notochord remnants."

    The rule: Include at least ONE specific anchor (organ system, location, imaging
    modality, patient population, or distinguishing feature) in the non-clozed text.

    ============================================================
    BAD EXAMPLES (avoid these patterns)
    ============================================================
    - "The answer is {{{{c1::C}}}}." (trivial, no educational value)
    - "{{{{c1::This}}}} is a common finding." (orphan cloze, no context)
    - "The {{{{c1::patient}}}} had symptoms." (generic, not specific)
    - "{{{{c1::Radiographs}}}} detect bone loss." (missing hint and bold)
    - "The {{{{c1::foramen spinosum::structure}}}} transmits the {{{{c2::middle meningeal artery::vessel}}}}." (not bolded)
    - Long sentence with buried cloze (keep it short, front-load context)
    - "<b>{{{{c1::X::diagnosis}}}}</b> is benign/malignant." (no organ system or context)

    ============================================================
    GUIDELINES
    ============================================================
    - Create as MANY cards as needed to capture ALL distinct facts
    - Prefer multiple simple cards over fewer complex cards
    - Each card should stand alone without the original question
    - Skip only: citations, references, question-specific phrasing ("In this case...")
    - When in doubt, make a card - more coverage is better

    Return ONLY a JSON array (no other text):
    [
      {{
        "cloze_text": "The <b>{{{{c1::foramen spinosum::structure}}}}</b> transmits the <b>{{{{c2::middle meningeal artery::vessel}}}}</b>.",
        "learning_point": "Foramen spinosum anatomy",
        "confidence": "high",
        "category": "anatomy"
      }},
      {{
        "cloze_text": "<b>{{{{c1::Mazabraud syndrome::syndrome}}}}</b> combines <b>{{{{c2::intramuscular myxomas::condition}}}}</b> and <b>{{{{c3::fibrous dysplasia::condition}}}}</b>.",
        "learning_point": "Mazabraud syndrome components",
        "confidence": "high",
        "category": "pathology"
      }},
      {{
        "cloze_text": "<b>Brainstem gliomas</b> account for <b>{{{{c1::10-20%::percentage}}}}</b> of pediatric <b>intracranial tumors</b>.",
        "learning_point": "Brainstem glioma epidemiology",
        "confidence": "high",
        "category": "statistics"
      }}
    ]

    Categories: anatomy, pathology, imaging, clinical, differential, statistics, mechanism

    If no suitable content for cloze cards, return: []

identify_question_blocks:
  description: "Identify question BLOCKS (grouped by main question number) with line ranges"
  prompt: |
    Identify ALL question blocks in this chapter, starting from block 1.

    TEXT FORMAT:
    - Each line starts with [LINE:NNNN] marker
    - Page breaks appear as [PAGE N] - content may span multiple pages
    - Questions section comes first, then answers section

    WHAT IS A BLOCK?
    A block groups related content by main question number:
    - Block "1" = all content for question 1 (context + 1a + 1b + 1c, etc.)
    - Block "2" = all content for question 2 (context + 2a + 2b, etc.)
    - Standalone questions (no sub-parts) are their own block

    QUESTION SECTION (what to capture in question_start to question_end):
    - Clinical context/scenario (e.g., "A 55-year-old male presents with...")
    - All sub-questions for this block (1a, 1b, 1c, etc.)
    - ALL answer choices (A, B, C, D) for each sub-question
    - IMPORTANT: Choices may appear on lines AFTER the question text, possibly across page breaks
    - question_end should be the LAST line containing choices for this block, just before the next block's context begins

    ANSWER SECTION (what to capture in answer_start to answer_end):
    The answer section contains TWO types of content - capture ALL of it:
    1. SPECIFIC ANSWERS: Lines like "1a Answer B." or "Answer 1a. B." with brief explanations
    2. SHARED DISCUSSION: Additional educational content that follows, including:
       - "Imaging Findings:" sections
       - "Discussion:" sections
       - "Differential Diagnosis:" sections
       - "References:" sections
       - Any other explanatory text before the next block's answers begin

    HOW TO FIND BOUNDARIES:
    - question_start: First line of this block (usually the main number with context, e.g., "1  A 55-year-old...")
    - question_end: Last line of choices before the NEXT block's number appears
    - answer_start: First line of answers for this block (e.g., "1a  Answer B." or "Answer 1a.")
    - answer_end: Last line before the next block's answer section begins (include all discussion content)

    CRITICAL: Start from block 1. Do not skip any blocks. Scan from the beginning of the text.

    OUTPUT FORMAT - one line per block, pipe-delimited with ONLY integers:
    block_id|question_start|question_end|answer_start|answer_end

    IMPORTANT: Output ONLY the line NUMBERS (integers), not the line content.
    Extract the number from [LINE:NNNN] markers - output just NNNN as an integer.

    Example: If block 1 starts at [LINE:0005] and ends at [LINE:0015], with answers from [LINE:0288] to [LINE:0321]:
    1|5|15|288|321

    Use 0 for answer_start and answer_end if no answer section exists for a block.
    Output ONLY the pipe-delimited lines with integers, nothing else.

    CHAPTER TEXT:
    {chapter_text}

format_raw_block:
  description: "Format a raw question/answer block into structured JSON"
  prompt: |
    Parse this raw question block into structured JSON.

    BLOCK ID: {block_id}

    RAW QUESTION TEXT:
    {question_text}

    RAW ANSWER TEXT:
    {answer_text}

    PARSING RULES:

    1. MARKERS TO UNDERSTAND:
       - [LINE:NNNN] markers: Ignore these, they are line number metadata
       - [PAGE N] markers: These indicate page breaks - extract page numbers for questions/answers

    2. FIND THE QUESTION IDENTIFIERS in the text:
       - Look for numbers like "1", "2", "3" OR sub-question IDs like "1a", "1b", "2a", "2b"
       - These may appear on their OWN LINE, separate from the question text
       - Example: "...decreased susceptibility\n1\nartifact?" means question ID is "1"
       - Example: "What is the diagnosis?\n2a\nA. Option..." means question ID is "2a"

    3. DETERMINE BLOCK TYPE:
       - STANDALONE: Block has only one question (e.g., just "1" or "3")
       - MULTI-PART: Block has sub-questions (e.g., "2a", "2b" under context "2")

    4. EXTRACT CONTEXT (for multi-part blocks):
       - Text that introduces the scenario BEFORE the first sub-question
       - Example: "A patient presents for arthrogram..." before "2a What is..."

    5. FOR EACH QUESTION/SUB-QUESTION, extract:
       - local_id: The ACTUAL ID from the text (e.g., "1", "2a", "3", "15b")
       - question_text: The question WITHOUT choices
       - choices: The A/B/C/D options
       - correct_answer: The letter (find in answer text: "Answer A" or "2a Answer B")
       - explanation: The specific explanation for THIS question

    6. IMAGE DISTRIBUTION ([IMAGE: filename.jpeg] markers):
       Images appear in TWO sections - track them separately:

       A. QUESTION IMAGES (from RAW QUESTION TEXT):
          - CONTEXT IMAGES: Before the first sub-question (in scenario/case description)
          - SUB-QUESTION IMAGES: Within or immediately after a specific sub-question

          Assign to each sub-question's `image_files` array:
          - Each sub-question gets: ALL context images + that sub-question's specific images
          - Example: context has [img1.jpg], sub-question 2b has [img2.jpg]:
            - 2a.image_files = ["img1.jpg"]  (context only)
            - 2b.image_files = ["img1.jpg", "img2.jpg"]  (context + specific)

       B. ANSWER IMAGES (from RAW ANSWER TEXT):
          - Images in the answer/explanation section
          - These should ONLY appear in explanations, not with the question

          Assign to each sub-question's `answer_image_files` array:
          - If an answer image relates to a specific sub-question, add it there
          - If it's in shared_discussion, add to `shared_discussion.image_files`

       When in doubt:
       - Question section images → treat as context (all sub-questions)
       - Answer section images → put in shared_discussion.image_files

    7. SHARED DISCUSSION (from answer section):
       - Imaging Findings, Discussion, Differential Diagnosis, References
       - This content applies to ALL questions in the block

    8. PAGE NUMBERS (from [PAGE N] markers):
       - Look for [PAGE N] markers in the text to determine page numbers
       - question_pages: List of page numbers where the question appears (from RAW QUESTION TEXT)
       - answer_pages: List of page numbers where the answer appears (from RAW ANSWER TEXT)
       - If a sub-question spans multiple pages, include all page numbers
       - If no [PAGE N] marker is found, use an empty array

    OUTPUT FORMAT - Return ONLY valid JSON:
    {{
      "block_id": "{block_id}",
      "context": {{
        "text": "Shared scenario text (empty string if standalone question)",
        "image_files": ["context_image.jpeg"]
      }},
      "sub_questions": [
        {{
          "local_id": "2a",
          "question_text": "Question text only, no choices",
          "choices": {{"A": "Option A", "B": "Option B", "C": "Option C", "D": "Option D"}},
          "correct_answer": "B",
          "explanation": "Brief explanation for this specific question",
          "image_files": ["question_image.jpeg"],
          "answer_image_files": ["answer_image.jpeg"],
          "question_pages": [14, 15],
          "answer_pages": [42]
        }}
      ],
      "shared_discussion": {{
        "imaging_findings": "Imaging findings text if present",
        "discussion": "Discussion text if present",
        "differential_diagnosis": "Differential diagnosis text if present",
        "references": ["Reference 1", "Reference 2"],
        "full_text": "All additional educational content",
        "image_files": ["shared_answer_image.jpeg"]
      }}
    }}

    CRITICAL:
    - Use the ACTUAL question ID from the text, not just the block_id
    - For standalone questions, sub_questions has ONE entry with local_id = the question number
    - Copy text VERBATIM (except for line markers)

generate_cloze_cards_from_block:
  description: "Generate cloze cards from an entire question block with full context"
  prompt: |
    You are creating Anki cloze deletion flashcards from a medical textbook question block.

    This is a COMPLETE block containing all related content - use ALL of it.

    BLOCK ID: {block_id}
    CHAPTER: {chapter_num}

    CONTEXT (clinical scenario):
    {context_text}

    SUB-QUESTIONS AND SPECIFIC ANSWERS:
    {sub_questions_text}

    SHARED DISCUSSION (applies to all sub-questions):
    {shared_discussion_text}

    ============================================================
    TASK: Create cloze cards from ALL content in this block
    ============================================================

    You have access to:
    1. The clinical scenario/context
    2. Each sub-question with its specific answer
    3. Shared discussion content (imaging findings, differential diagnosis, etc.)

    Extract facts from ALL of these sources. The shared discussion is especially valuable
    as it contains detailed educational content that applies to the entire case.

    For each card, indicate which sub-question it relates to most (if applicable).

    CLOZE SYNTAX:
    - Use {{{{c1::text::hint}}}} format with semantic hints
    - BOLD all clozes using <b> tags
    - BOLD important terms for visual emphasis
    - Use c1, c2, c3 on same card for related facts from same sentence
    - Create separate cards for unrelated facts

    Return ONLY a JSON array:
    [
      {{
        "cloze_text": "<b>{{{{c1::Finding::type}}}}</b> is seen in this <b>condition</b>.",
        "learning_point": "Key concept",
        "confidence": "high",
        "category": "imaging",
        "source_sub_question_id": "1a"
      }},
      ...
    ]

    source_sub_question_id should be:
    - The specific sub-question ID if the fact relates to a specific answer
    - Empty string "" if the fact comes from shared discussion applicable to all

    Categories: anatomy, pathology, imaging, clinical, differential, statistics, mechanism

    If no suitable content, return: []
