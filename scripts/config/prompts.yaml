# LLM Prompts for Textbook Q&A Extractor
# Edit these prompts to customize extraction behavior
# Variables use {variable_name} syntax and are filled in at runtime

identify_chapters:
  description: "Identify chapter boundaries from PDF page index"
  prompt: |
    Analyze this textbook page index and identify all chapters that contain QUESTIONS sections.

    This is a medical textbook where each chapter has:
    - A chapter header (e.g., "Chapter 1: Title" or "1 Title")
    - A QUESTIONS section with numbered questions
    - An ANSWERS section with explanations

    For each chapter with questions, provide:
    - chapter_number: The chapter number (integer)
    - title: The chapter title
    - start_page: The page number where the chapter starts (from the [PAGE X] markers)
    - has_questions: true (only include chapters that have questions)

    Look for patterns like "QUESTIONS" or numbered questions (1., 2., etc.) to identify which chapters have Q&A content.

    Return ONLY a JSON array, no other text:
    [
      {{"chapter_number": 1, "title": "Chapter Title", "start_page": 14, "has_questions": true}},
      ...
    ]

    PAGE INDEX:
    {page_index}

extract_qa_pairs:
  description: "Extract Q&A pairs from a single chapter"
  prompt: |
    You are analyzing Chapter {chapter_num} of a medical textbook to extract all questions and their corresponding answers.

    TASK:
    1. Find all questions in the QUESTIONS section
    2. Find all answers in the ANSWERS section
    3. Match each question to its answer
    4. Identify the correct answer choice (A, B, C, D, or E)
    5. Identify shared context relationships between questions

    CRITICAL - VERBATIM EXTRACTION:
    You MUST copy all text EXACTLY as it appears in the source. Do NOT paraphrase, summarize, or shorten:
    - Question text: Copy the COMPLETE question text word-for-word
    - Answer choices: Copy each choice (A, B, C, D, E) EXACTLY as written
    - Explanations: Copy the COMPLETE explanation/answer text VERBATIM from the ANSWERS section

    Do NOT truncate or abbreviate any text. Include every word from the original.

    SHARED CONTEXT DETECTION:
    Some questions share context - a clinical scenario, case description, or introductory text that
    applies to multiple follow-up questions. Identify these relationships:

    - PATTERN 1: Explicit multi-part (e.g., "1" provides context, "1a", "1b", "1c" are questions)
      The context entry has no answer choices. Sub-questions reference it.

    - PATTERN 2: Implicit shared context (e.g., "Questions 5-7 refer to the following case...")
      Multiple standalone questions share the same introductory scenario.

    - PATTERN 3: Sequential related questions (e.g., Q2 says "In the same patient..." referring to Q1)
      Later questions explicitly reference earlier ones.

    For each question, set context_source:
    - null if the question is standalone (no shared context)
    - The question ID that provides context (e.g., "1" for question "1a")

    ADDITIONAL RULES:
    - Questions may have sub-parts like 2a, 2b, 2c - treat each as a separate question
    - Question IDs should match exactly as they appear (e.g., "1", "1a", "1b", "2a", "2b", "3")
    - For images: If a question references an image (e.g., "image below", "figure", "radiograph shown"), mark has_image: true
    - Use image_group to indicate which questions share the same image (e.g., "1" for questions 1, 1a, 1b, 1c sharing one image)

    Return ONLY a JSON object in this exact format:
    {{
      "chapter": {chapter_num},
      "questions": [
        {{
          "id": "1",
          "text": "Complete clinical scenario/context text copied verbatim",
          "choices": {{}},
          "has_image": true,
          "image_group": "1",
          "correct_answer": "",
          "explanation": "",
          "context_source": null
        }},
        {{
          "id": "1a",
          "text": "Complete question text copied verbatim",
          "choices": {{
            "A": "Complete choice A text verbatim",
            "B": "Complete choice B text verbatim",
            "C": "Complete choice C text verbatim",
            "D": "Complete choice D text verbatim"
          }},
          "has_image": false,
          "image_group": "1",
          "correct_answer": "B",
          "explanation": "Complete explanation copied verbatim from the ANSWERS section",
          "context_source": "1"
        }},
        {{
          "id": "2",
          "text": "Standalone question text copied verbatim",
          "choices": {{
            "A": "Choice A",
            "B": "Choice B",
            "C": "Choice C",
            "D": "Choice D"
          }},
          "has_image": false,
          "image_group": null,
          "correct_answer": "A",
          "explanation": "Explanation copied verbatim",
          "context_source": null
        }}
      ]
    }}

    CHAPTER TEXT:
    {chapter_text}

match_images_to_questions:
  description: "Match images to questions based on flanking text context"
  prompt: |
    Match images to questions for Chapter {chapter_num}.

    TEXTBOOK STRUCTURE (critical for correct matching):
    In this textbook, each question appears in this order:
    1. Question text ending with the question number (e.g., "...needle placement? 2a")
    2. IMAGE(S) - one OR MORE images may appear here
    3. Answer choices (A, B, C, D)
    4. Next question...

    Therefore: The question number at the END of "Text BEFORE image" tells you which question the image belongs to.

    QUESTIONS:
    {questions_text}

    IMAGES (with surrounding text context):
    {images_text}

    MATCHING RULES:
    1. Find the LAST question number mentioned in "Text BEFORE image" - that's the question this image belongs to
    2. Example: If text before ends with "...rotator interval approach? 2a" -> image belongs to ch{chapter_num}_2a
    3. Example: If text before ends with "...mixture for the arthrogram? 2b" -> image belongs to ch{chapter_num}_2b
    4. The text AFTER typically shows the answer choices, then the NEXT question
    5. MULTIPLE IMAGES CAN BELONG TO THE SAME QUESTION - if two or more images have the same question number in their "Text BEFORE", assign ALL of them to that question
    6. Example: If two images both have text before ending with "...image of the foot? 40" -> BOTH images belong to ch{chapter_num}_40
    7. Do NOT share a single image across multiple different questions
    8. Decorative images or images not matching any question -> assign to "(none)"

    Return ONLY a JSON object mapping image filenames to question IDs:
    {{
      "image1.jpeg": "ch{chapter_num}_40",
      "image2.jpeg": "ch{chapter_num}_40",
      "another_image.jpeg": "(none)"
    }}

postprocess_questions:
  description: "Link context questions to their sub-questions"
  prompt: |
    Analyze this list of extracted questions and identify context relationships.

    TASK:
    1. Identify "context-only" entries - these have descriptive text but NO answer choices (empty choices dict)
    2. For each context-only entry, find its related sub-questions by matching the image_group
    3. Return the updated questions with context properly linked

    RULES:
    - A question is "context-only" if it has no choices (choices is empty {{}}) AND its ID is just a number (e.g., "5") not a letter suffix (e.g., "5a")
    - Sub-questions share the same image_group as their context question
    - Example: If question "5" has image_group="5" and no choices, and questions "5a", "5b", "5c" also have image_group="5", then 5a/5b/5c are sub-questions of context "5"

    FOR EACH QUESTION, add these fields:
    - "is_context_only": true/false - true if this is just context (no choices, no correct answer)
    - "context": "" - for sub-questions, copy the context question's text here. Leave empty for standalone questions.
    - "context_question_id": "" - for sub-questions, the local_id of their context question (e.g., "5"). Leave empty otherwise.

    QUESTIONS TO PROCESS:
    {questions_json}

    Return ONLY the updated JSON array with all original fields preserved plus the new fields added.

associate_context:
  description: "Identify context relationships and merge context into sub-questions"
  prompt: |
    Analyze these questions from a medical textbook and identify CONTEXT relationships.

    WHAT IS A CONTEXT RELATIONSHIP?
    Sometimes a clinical scenario, case description, or introductory paragraph provides context
    that multiple follow-up questions refer to. The context entry itself is NOT a standalone
    question - it provides background information that the actual questions build upon.

    AVAILABLE DATA FOR EACH QUESTION:
    - full_id: Complete question identifier (e.g., "ch1_5")
    - local_id: Question number within chapter (e.g., "5", "5a")
    - text_preview: First 300 characters of the question text
    - has_choices: Whether the question has A/B/C/D answer options
    - num_choices: Number of answer choices (0-5)
    - has_correct_answer: Whether a correct answer was identified
    - has_explanation: Whether an explanation/answer text exists

    ABSOLUTE RULE - NEVER VIOLATE THIS:
    A question with has_choices: true can NEVER be a context_id.
    Context entries MUST have has_choices: false and num_choices: 0.
    If a question has answer choices (A/B/C/D), it is a REAL QUESTION, not context.

    COMMON MISTAKE TO AVOID:
    Questions like "2a" and "2b" are often BOTH real questions that happen to share a clinical
    scenario in their text. Do NOT assume "2a" is context for "2b" just because of the naming.
    Look at has_choices - if BOTH have choices, NEITHER is context. They are independent questions.

    VALID CONTEXT PATTERN:
    - Question "5" has has_choices: false, num_choices: 0 (just a scenario, no A/B/C/D)
    - Questions "5a", "5b", "5c" have has_choices: true (actual questions with choices)
    - In this case, "5" IS context for "5a", "5b", "5c"

    INVALID - DO NOT CREATE:
    - Question "2a" has has_choices: true, num_choices: 4
    - Question "2b" has has_choices: true, num_choices: 4
    - Both are real questions. Do NOT mark "2a" as context for "2b".

    QUESTIONS:
    {questions_summary}

    Return a JSON object:
    {{
      "context_mappings": [
        {{
          "context_id": "ch1_5",
          "sub_question_ids": ["ch1_5a", "ch1_5b", "ch1_5c"]
        }}
      ]
    }}

    If there are no context relationships, return: {{"context_mappings": []}}

    Return ONLY the JSON, no other text.

extract_line_ranges:
  description: "First pass: identify line ranges for each Q&A pair (minimal output)"
  prompt: |
    Analyze this chapter text and identify all question-answer pairs.

    The text has line numbers in format [LINE:NNNN]. Images are marked as [IMAGE: filename].

    TEXTBOOK STRUCTURE:
    - QUESTIONS section: Contains numbered questions with answer choices (A/B/C/D/E)
    - ANSWERS section: Contains explanations starting with "Answer X." where X is the correct letter
    - Questions and answers are in the same order (Question 1 matches Answer 1, etc.)

    CRITICAL - IMAGES AND CHOICES:
    - Images appear BETWEEN the question text and the answer choices
    - The typical structure is: Question text → [IMAGE] → A. choice → B. choice → C. choice → D. choice
    - You MUST include the answer choices (A/B/C/D) in question_end, even when an image appears before them
    - question_end should be the line number of the LAST choice (D or E), NOT the line before the image

    SHARED CONTEXT DETECTION:
    Some questions share context - a clinical scenario, case description, or introductory text that
    applies to multiple follow-up questions. Identify these relationships:

    - PATTERN 1: Explicit multi-part (e.g., "1" provides context, "1a", "1b", "1c" are questions)
      The context entry has no answer choices. Sub-questions reference it.

    - PATTERN 2: Implicit shared context (e.g., "Questions 5-7 refer to the following case...")
      Multiple standalone questions share the same introductory scenario.

    - PATTERN 3: Sequential related questions (e.g., Q2 says "In the same patient..." referring to Q1)
      Later questions explicitly reference earlier ones.

    For each question, determine:
    - context_source: The question_id that provides context for THIS question, or null if standalone
    - Example: "1a" might have context_source: "1" if question "1" provides the scenario
    - Example: "6" might have context_source: "5" if Q5 has shared context for Q5-7

    For EACH question, output:
    - question_id: The question number as it appears (e.g., "1", "1a", "2b", "15")
    - question_start: Line number where question text begins
    - question_end: Line number of the LAST answer choice (D or E line), or last line of question text if no choices
    - answer_start: Line number where answer/explanation begins (0 if no answer)
    - answer_end: Line number where answer ends (0 if no answer)
    - correct_letter: The correct answer letter (A/B/C/D/E), or "" for context-only entries
    - image_files: Array of image filenames that appear within this question's line range
    - context_source: The question_id providing context for this question, or null if standalone

    Return ONLY a JSON array:
    [
      {{
        "question_id": "1",
        "question_start": 5,
        "question_end": 8,
        "answer_start": 0,
        "answer_end": 0,
        "correct_letter": "",
        "image_files": ["p014_y0321_x0172_0.jpeg"],
        "context_source": null
      }},
      {{
        "question_id": "1a",
        "question_start": 9,
        "question_end": 14,
        "answer_start": 450,
        "answer_end": 460,
        "correct_letter": "B",
        "image_files": [],
        "context_source": "1"
      }},
      {{
        "question_id": "2",
        "question_start": 15,
        "question_end": 22,
        "answer_start": 461,
        "answer_end": 475,
        "correct_letter": "A",
        "image_files": [],
        "context_source": null
      }},
      ...
    ]

    CHAPTER TEXT:
    {chapter_text}

format_qa_pair:
  description: "Second pass: format a single Q&A pair into structured JSON"
  prompt: |
    Extract and format this question-answer pair from a medical textbook.

    QUESTION ID: {question_id}

    QUESTION TEXT (copy verbatim):
    {question_text}

    ANSWER TEXT (copy verbatim):
    {answer_text}

    INSTRUCTIONS:
    1. Copy ALL text EXACTLY as it appears - do not paraphrase or shorten
    2. Extract the question text (before the choices)
    3. Extract each answer choice (A, B, C, D, and E if present)
    4. Copy the full explanation verbatim
    5. If this is a context-only question (no choices), leave choices empty and mark is_context_only: true

    Return ONLY a JSON object:
    {{
      "id": "{question_id}",
      "text": "Full question text copied verbatim",
      "choices": {{
        "A": "Choice A text verbatim",
        "B": "Choice B text verbatim",
        "C": "Choice C text verbatim",
        "D": "Choice D text verbatim"
      }},
      "has_image": true,
      "image_group": "1",
      "correct_answer": "A",
      "explanation": "Full explanation copied verbatim",
      "is_context_only": false
    }}

    For context-only questions (no choices):
    {{
      "id": "{question_id}",
      "text": "Clinical scenario text copied verbatim",
      "choices": {{}},
      "has_image": true,
      "image_group": "1",
      "correct_answer": "",
      "explanation": "",
      "is_context_only": true
    }}

generate_cloze_cards:
  description: "Generate cloze deletion flashcards from explanation text"
  prompt: |
    You are creating Anki cloze deletion flashcards from a medical textbook explanation.

    EXPLANATION TEXT (from {question_id}):
    {explanation}

    TASK: Extract ALL distinct facts and concepts as cloze deletion cards.

    ============================================================
    CORE PRINCIPLE: MINIMUM INFORMATION
    ============================================================
    Each card tests ONE atomic fact. If a sentence contains multiple learnable facts,
    create MULTIPLE cards. The source material is extremely high-density - err on the
    side of creating MORE cards rather than fewer. A single explanation may yield
    1-2 cards (if brief) or 20+ cards (if detailed). Let the content dictate the count.

    Card quality checklist:
    - Does this card test exactly ONE fact? If not, split it.
    - Is the sentence as SHORT as possible while retaining meaning?
    - Does the context constrain the answer to one reasonable response?
    - Would a student need to truly recall this, not just recognize it?

    ============================================================
    CLOZE SYNTAX RULES (NOTES vs CARDS)
    ============================================================
    IMPORTANT: Understand the difference between NOTES and CARDS in Anki:
    - A NOTE is one item in your JSON output (one cloze_text)
    - A note with {{{{c1::X}}}} and {{{{c2::Y}}}} generates TWO CARDS automatically
    - Card 1 hides X (shows Y), Card 2 hides Y (shows X)

    RULE: If multiple testable facts come from the SAME sentence or tightly related
    concept, put them on ONE note using c1, c2, c3. Do NOT create separate notes.

    BAD - Three separate notes for related facts:
    Note 1: "<b>{{{{c1::Mazabraud syndrome::syndrome}}}}</b> combines intramuscular myxomas and fibrous dysplasia."
    Note 2: "Mazabraud syndrome combines <b>{{{{c1::intramuscular myxomas::condition}}}}</b> and fibrous dysplasia."
    Note 3: "Mazabraud syndrome combines intramuscular myxomas and <b>{{{{c1::fibrous dysplasia::condition}}}}</b>."

    GOOD - One note with three clozes (generates 3 cards):
    "<b>{{{{c1::Mazabraud syndrome::syndrome}}}}</b> combines <b>{{{{c2::intramuscular myxomas::condition}}}}</b> and <b>{{{{c3::fibrous dysplasia::condition}}}}</b>."

    SYNTAX RULES:
    1. Use {{{{c1::text::hint}}}} format - ALWAYS include a semantic hint
    2. BOLD all cloze deletions using <b> tags
    3. BOLD important non-clozed terms (diagnoses, structures, key findings) for visual emphasis
    4. Use c1, c2, c3 on SAME note for facts from the same sentence/concept
    5. Create SEPARATE notes only for unrelated facts or different sentences
    6. Keep sentences SHORT - front-load the topic/category for context

    HINT CATEGORIES (use these or similar):
    - Anatomical: structure, location, region, pathway, vessel, nerve, muscle, bone
    - Clinical: finding, symptom, sign, presentation, complication
    - Diagnostic: technique, modality, imaging finding, test, criteria
    - Pathology: condition, disease, mechanism, etiology, cause
    - Quantitative: percentage, number, measurement, duration, age, frequency
    - Comparative: difference, feature, characteristic, distinguishing feature

    ============================================================
    WHAT TO EXTRACT (create a card for each)
    ============================================================
    - Every specific term, name, or diagnosis mentioned
    - Every number, percentage, or statistic
    - Every anatomical relationship (X is located in Y, X innervates Y)
    - Every causal relationship (X causes Y, X leads to Y)
    - Every distinguishing feature in differentials
    - Every imaging finding or diagnostic criterion
    - Every mechanism or pathway step
    - Every clinical presentation or symptom association
    - Every treatment or management point

    ============================================================
    GOOD EXAMPLES (with proper note structure and bolding)
    ============================================================
    SINGLE-CLOZE NOTES (for isolated facts):
    - "<b>Brainstem gliomas</b> account for <b>{{{{c1::10-20%::percentage}}}}</b> of pediatric intracranial tumors."
    - "<b>{{{{c1::DNET::diagnosis}}}}</b> most commonly occurs in the <b>temporal lobe</b>."
    - "<b>Radiography</b> is <b>{{{{c1::insensitive::limitation}}}}</b> for detecting early <b>bone loss</b>."

    MULTI-CLOZE NOTES (for related facts from same concept - generates multiple cards):
    - "The <b>{{{{c1::foramen spinosum::structure}}}}</b> transmits the <b>{{{{c2::middle meningeal artery::vessel}}}}</b>."
      (One note → 2 cards: one tests structure, one tests vessel)

    - "<b>{{{{c1::DNET::diagnosis}}}}</b> presents with <b>{{{{c2::intractable seizures::symptom}}}}</b> in the <b>{{{{c3::temporal lobe::location}}}}</b>."
      (One note → 3 cards)

    - "<b>{{{{c1::Ganglioglioma::diagnosis}}}}</b> differs from <b>DNET</b> by more frequent <b>{{{{c2::calcification::finding}}}}</b> and <b>{{{{c3::contrast enhancement::finding}}}}</b>."
      (One note → 3 cards, with DNET bolded but not clozed for context)

    - "<b>{{{{c1::Intraosseous lipomas::diagnosis}}}}</b> are <b>{{{{c2::benign::behavior}}}}</b> fatty tumors of <b>bone</b>."
      (One note → 2 cards)

    Notice: Bold ALL key medical terms for visual scanning, whether clozed or not.
    When facts are from the same sentence, combine into one note with c1, c2, c3.

    ============================================================
    CRITICAL: STANDALONE CONTEXT
    ============================================================
    Cards will be reviewed randomly mixed with cards from OTHER topics and chapters.
    The NON-clozed text must provide enough context to identify the domain/topic.

    Ask yourself: "If a student sees this card mixed with 1000 other cards from
    different subjects, can they tell what field/topic this is about?"

    BAD - No context to constrain the answer:
    - "<b>{{{{c1::Intraosseous lipomas::diagnosis}}}}</b> are benign lesions."
      Problem: "[diagnosis] are benign lesions" - hundreds of things are benign!
      The student has no idea this is about bone tumors.

    GOOD - Context anchors the topic:
    - "<b>{{{{c1::Intraosseous lipomas::diagnosis}}}}</b> are benign fatty bone tumors."
    - "In bone radiology, <b>{{{{c1::intraosseous lipomas::diagnosis}}}}</b> are benign lesions."

    BAD - Generic context:
    - "<b>{{{{c1::Chordoma::diagnosis}}}}</b> is a malignant tumor."
      Problem: Malignant tumor of what? Brain? Bone? Soft tissue?

    GOOD - Specific context:
    - "<b>{{{{c1::Chordoma::diagnosis}}}}</b> is a malignant tumor arising from notochord remnants."
    - "Sacral/clival tumors: <b>{{{{c1::Chordoma::diagnosis}}}}</b> arises from notochord remnants."

    The rule: Include at least ONE specific anchor (organ system, location, imaging
    modality, patient population, or distinguishing feature) in the non-clozed text.

    ============================================================
    BAD EXAMPLES (avoid these patterns)
    ============================================================
    - "The answer is {{{{c1::C}}}}." (trivial, no educational value)
    - "{{{{c1::This}}}} is a common finding." (orphan cloze, no context)
    - "The {{{{c1::patient}}}} had symptoms." (generic, not specific)
    - "{{{{c1::Radiographs}}}} detect bone loss." (missing hint and bold)
    - "The {{{{c1::foramen spinosum::structure}}}} transmits the {{{{c2::middle meningeal artery::vessel}}}}." (not bolded)
    - Long sentence with buried cloze (keep it short, front-load context)
    - "<b>{{{{c1::X::diagnosis}}}}</b> is benign/malignant." (no organ system or context)

    ============================================================
    GUIDELINES
    ============================================================
    - Create as MANY cards as needed to capture ALL distinct facts
    - Prefer multiple simple cards over fewer complex cards
    - Each card should stand alone without the original question
    - Skip only: citations, references, question-specific phrasing ("In this case...")
    - When in doubt, make a card - more coverage is better

    Return ONLY a JSON array (no other text):
    [
      {{
        "cloze_text": "The <b>{{{{c1::foramen spinosum::structure}}}}</b> transmits the <b>{{{{c2::middle meningeal artery::vessel}}}}</b>.",
        "learning_point": "Foramen spinosum anatomy",
        "confidence": "high",
        "category": "anatomy"
      }},
      {{
        "cloze_text": "<b>{{{{c1::Mazabraud syndrome::syndrome}}}}</b> combines <b>{{{{c2::intramuscular myxomas::condition}}}}</b> and <b>{{{{c3::fibrous dysplasia::condition}}}}</b>.",
        "learning_point": "Mazabraud syndrome components",
        "confidence": "high",
        "category": "pathology"
      }},
      {{
        "cloze_text": "<b>Brainstem gliomas</b> account for <b>{{{{c1::10-20%::percentage}}}}</b> of pediatric <b>intracranial tumors</b>.",
        "learning_point": "Brainstem glioma epidemiology",
        "confidence": "high",
        "category": "statistics"
      }}
    ]

    Categories: anatomy, pathology, imaging, clinical, differential, statistics, mechanism

    If no suitable content for cloze cards, return: []
