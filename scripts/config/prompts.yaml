# LLM Prompts for Textbook Q&A Extractor
# Edit these prompts to customize extraction behavior
# Variables use {variable_name} syntax and are filled in at runtime

identify_chapters:
  description: "Identify chapter boundaries from PDF page index"
  prompt: |
    Analyze this textbook page index and identify all chapters that contain QUESTIONS sections.

    This is a medical textbook where each chapter has:
    - A chapter header (e.g., "Chapter 1: Title" or "1 Title")
    - A QUESTIONS section with numbered questions
    - An ANSWERS section with explanations

    For each chapter with questions, provide:
    - chapter_number: The chapter number (integer)
    - title: The chapter title
    - start_page: The page number where the chapter starts (from the [PAGE X] markers)
    - has_questions: true (only include chapters that have questions)

    Look for patterns like "QUESTIONS" or numbered questions (1., 2., etc.) to identify which chapters have Q&A content.

    Return ONLY a JSON array, no other text:
    [
      {{"chapter_number": 1, "title": "Chapter Title", "start_page": 14, "has_questions": true}},
      ...
    ]

    PAGE INDEX:
    {page_index}

extract_qa_pairs:
  description: "Extract Q&A pairs from a single chapter"
  prompt: |
    You are analyzing Chapter {chapter_num} of a medical textbook to extract all questions and their corresponding answers.

    TASK:
    1. Find all questions in the QUESTIONS section
    2. Find all answers in the ANSWERS section
    3. Match each question to its answer
    4. Identify the correct answer choice (A, B, C, D, or E)
    5. Identify shared context relationships between questions

    CRITICAL - VERBATIM EXTRACTION:
    You MUST copy all text EXACTLY as it appears in the source. Do NOT paraphrase, summarize, or shorten:
    - Question text: Copy the COMPLETE question text word-for-word
    - Answer choices: Copy each choice (A, B, C, D, E) EXACTLY as written
    - Explanations: Copy the COMPLETE explanation/answer text VERBATIM from the ANSWERS section

    Do NOT truncate or abbreviate any text. Include every word from the original.

    SHARED CONTEXT DETECTION:
    Some questions share context - a clinical scenario, case description, or introductory text that
    applies to multiple follow-up questions. Identify these relationships:

    - PATTERN 1: Explicit multi-part (e.g., "1" provides context, "1a", "1b", "1c" are questions)
      The context entry has no answer choices. Sub-questions reference it.

    - PATTERN 2: Implicit shared context (e.g., "Questions 5-7 refer to the following case...")
      Multiple standalone questions share the same introductory scenario.

    - PATTERN 3: Sequential related questions (e.g., Q2 says "In the same patient..." referring to Q1)
      Later questions explicitly reference earlier ones.

    For each question, set context_source:
    - null if the question is standalone (no shared context)
    - The question ID that provides context (e.g., "1" for question "1a")

    ADDITIONAL RULES:
    - Questions may have sub-parts like 2a, 2b, 2c - treat each as a separate question
    - Question IDs should match exactly as they appear (e.g., "1", "1a", "1b", "2a", "2b", "3")
    - For images: If a question references an image (e.g., "image below", "figure", "radiograph shown"), mark has_image: true
    - Use image_group to indicate which questions share the same image (e.g., "1" for questions 1, 1a, 1b, 1c sharing one image)

    Return ONLY a JSON object in this exact format:
    {{
      "chapter": {chapter_num},
      "questions": [
        {{
          "id": "1",
          "text": "Complete clinical scenario/context text copied verbatim",
          "choices": {{}},
          "has_image": true,
          "image_group": "1",
          "correct_answer": "",
          "explanation": "",
          "context_source": null
        }},
        {{
          "id": "1a",
          "text": "Complete question text copied verbatim",
          "choices": {{
            "A": "Complete choice A text verbatim",
            "B": "Complete choice B text verbatim",
            "C": "Complete choice C text verbatim",
            "D": "Complete choice D text verbatim"
          }},
          "has_image": false,
          "image_group": "1",
          "correct_answer": "B",
          "explanation": "Complete explanation copied verbatim from the ANSWERS section",
          "context_source": "1"
        }},
        {{
          "id": "2",
          "text": "Standalone question text copied verbatim",
          "choices": {{
            "A": "Choice A",
            "B": "Choice B",
            "C": "Choice C",
            "D": "Choice D"
          }},
          "has_image": false,
          "image_group": null,
          "correct_answer": "A",
          "explanation": "Explanation copied verbatim",
          "context_source": null
        }}
      ]
    }}

    CHAPTER TEXT:
    {chapter_text}

match_images_to_questions:
  description: "Match images to questions based on flanking text context"
  prompt: |
    Match images to questions for Chapter {chapter_num}.

    TEXTBOOK STRUCTURE (critical for correct matching):
    In this textbook, each question appears in this order:
    1. Question text ending with the question number (e.g., "...needle placement? 2a")
    2. IMAGE(S) - one OR MORE images may appear here
    3. Answer choices (A, B, C, D)
    4. Next question...

    Therefore: The question number at the END of "Text BEFORE image" tells you which question the image belongs to.

    QUESTIONS:
    {questions_text}

    IMAGES (with surrounding text context):
    {images_text}

    MATCHING RULES:
    1. Find the LAST question number mentioned in "Text BEFORE image" - that's the question this image belongs to
    2. Example: If text before ends with "...rotator interval approach? 2a" -> image belongs to ch{chapter_num}_2a
    3. Example: If text before ends with "...mixture for the arthrogram? 2b" -> image belongs to ch{chapter_num}_2b
    4. The text AFTER typically shows the answer choices, then the NEXT question
    5. MULTIPLE IMAGES CAN BELONG TO THE SAME QUESTION - if two or more images have the same question number in their "Text BEFORE", assign ALL of them to that question
    6. Example: If two images both have text before ending with "...image of the foot? 40" -> BOTH images belong to ch{chapter_num}_40
    7. Do NOT share a single image across multiple different questions
    8. Decorative images or images not matching any question -> assign to "(none)"

    Return ONLY a JSON object mapping image filenames to question IDs:
    {{
      "image1.jpeg": "ch{chapter_num}_40",
      "image2.jpeg": "ch{chapter_num}_40",
      "another_image.jpeg": "(none)"
    }}

postprocess_questions:
  description: "Link context questions to their sub-questions"
  prompt: |
    Analyze this list of extracted questions and identify context relationships.

    TASK:
    1. Identify "context-only" entries - these have descriptive text but NO answer choices (empty choices dict)
    2. For each context-only entry, find its related sub-questions by matching the image_group
    3. Return the updated questions with context properly linked

    RULES:
    - A question is "context-only" if it has no choices (choices is empty {{}}) AND its ID is just a number (e.g., "5") not a letter suffix (e.g., "5a")
    - Sub-questions share the same image_group as their context question
    - Example: If question "5" has image_group="5" and no choices, and questions "5a", "5b", "5c" also have image_group="5", then 5a/5b/5c are sub-questions of context "5"

    FOR EACH QUESTION, add these fields:
    - "is_context_only": true/false - true if this is just context (no choices, no correct answer)
    - "context": "" - for sub-questions, copy the context question's text here. Leave empty for standalone questions.
    - "context_question_id": "" - for sub-questions, the local_id of their context question (e.g., "5"). Leave empty otherwise.

    QUESTIONS TO PROCESS:
    {questions_json}

    Return ONLY the updated JSON array with all original fields preserved plus the new fields added.

associate_context:
  description: "Identify context relationships and merge context into sub-questions"
  prompt: |
    Analyze these questions from a medical textbook and identify CONTEXT relationships.

    WHAT IS A CONTEXT RELATIONSHIP?
    Sometimes a clinical scenario, case description, or introductory paragraph provides context
    that multiple follow-up questions refer to. The context entry itself is NOT a standalone
    question - it provides background information that the actual questions build upon.

    AVAILABLE DATA FOR EACH QUESTION:
    - full_id: Complete question identifier (e.g., "ch1_5")
    - local_id: Question number within chapter (e.g., "5", "5a")
    - text_preview: First 300 characters of the question text
    - has_choices: Whether the question has A/B/C/D answer options
    - num_choices: Number of answer choices (0-5)
    - has_correct_answer: Whether a correct answer was identified
    - has_explanation: Whether an explanation/answer text exists

    ABSOLUTE RULE - NEVER VIOLATE THIS:
    A question with has_choices: true can NEVER be a context_id.
    Context entries MUST have has_choices: false and num_choices: 0.
    If a question has answer choices (A/B/C/D), it is a REAL QUESTION, not context.

    COMMON MISTAKE TO AVOID:
    Questions like "2a" and "2b" are often BOTH real questions that happen to share a clinical
    scenario in their text. Do NOT assume "2a" is context for "2b" just because of the naming.
    Look at has_choices - if BOTH have choices, NEITHER is context. They are independent questions.

    VALID CONTEXT PATTERN:
    - Question "5" has has_choices: false, num_choices: 0 (just a scenario, no A/B/C/D)
    - Questions "5a", "5b", "5c" have has_choices: true (actual questions with choices)
    - In this case, "5" IS context for "5a", "5b", "5c"

    INVALID - DO NOT CREATE:
    - Question "2a" has has_choices: true, num_choices: 4
    - Question "2b" has has_choices: true, num_choices: 4
    - Both are real questions. Do NOT mark "2a" as context for "2b".

    QUESTIONS:
    {questions_summary}

    Return a JSON object:
    {{
      "context_mappings": [
        {{
          "context_id": "ch1_5",
          "sub_question_ids": ["ch1_5a", "ch1_5b", "ch1_5c"]
        }}
      ]
    }}

    If there are no context relationships, return: {{"context_mappings": []}}

    Return ONLY the JSON, no other text.

extract_line_ranges:
  description: "First pass: identify line ranges for each Q&A pair (minimal output)"
  prompt: |
    Analyze this chapter text and identify all question-answer pairs.

    The text has line numbers in format [LINE:NNNN]. Images are marked as [IMAGE: filename].

    TEXTBOOK STRUCTURE:
    - QUESTIONS section: Contains numbered questions with answer choices (A/B/C/D/E)
    - ANSWERS section: Contains explanations starting with "Answer X." where X is the correct letter
    - Questions and answers are in the same order (Question 1 matches Answer 1, etc.)

    CRITICAL - IMAGES AND CHOICES:
    - Images appear BETWEEN the question text and the answer choices
    - The typical structure is: Question text → [IMAGE] → A. choice → B. choice → C. choice → D. choice
    - You MUST include the answer choices (A/B/C/D) in question_end, even when an image appears before them
    - question_end should be the line number of the LAST choice (D or E), NOT the line before the image

    SHARED CONTEXT DETECTION:
    Some questions share context - a clinical scenario, case description, or introductory text that
    applies to multiple follow-up questions. Identify these relationships:

    - PATTERN 1: Explicit multi-part (e.g., "1" provides context, "1a", "1b", "1c" are questions)
      The context entry has no answer choices. Sub-questions reference it.

    - PATTERN 2: Implicit shared context (e.g., "Questions 5-7 refer to the following case...")
      Multiple standalone questions share the same introductory scenario.

    - PATTERN 3: Sequential related questions (e.g., Q2 says "In the same patient..." referring to Q1)
      Later questions explicitly reference earlier ones.

    For each question, determine:
    - context_source: The question_id that provides context for THIS question, or null if standalone
    - Example: "1a" might have context_source: "1" if question "1" provides the scenario
    - Example: "6" might have context_source: "5" if Q5 has shared context for Q5-7

    For EACH question, output:
    - question_id: The question number as it appears (e.g., "1", "1a", "2b", "15")
    - question_start: Line number where question text begins
    - question_end: Line number of the LAST answer choice (D or E line), or last line of question text if no choices
    - answer_start: Line number where answer/explanation begins (0 if no answer)
    - answer_end: Line number where answer ends (0 if no answer)
    - correct_letter: The correct answer letter (A/B/C/D/E), or "" for context-only entries
    - image_files: Array of image filenames that appear within this question's line range
    - context_source: The question_id providing context for this question, or null if standalone

    Return ONLY a JSON array:
    [
      {{
        "question_id": "1",
        "question_start": 5,
        "question_end": 8,
        "answer_start": 0,
        "answer_end": 0,
        "correct_letter": "",
        "image_files": ["p014_y0321_x0172_0.jpeg"],
        "context_source": null
      }},
      {{
        "question_id": "1a",
        "question_start": 9,
        "question_end": 14,
        "answer_start": 450,
        "answer_end": 460,
        "correct_letter": "B",
        "image_files": [],
        "context_source": "1"
      }},
      {{
        "question_id": "2",
        "question_start": 15,
        "question_end": 22,
        "answer_start": 461,
        "answer_end": 475,
        "correct_letter": "A",
        "image_files": [],
        "context_source": null
      }},
      ...
    ]

    CHAPTER TEXT:
    {chapter_text}

format_qa_pair:
  description: "Second pass: format a single Q&A pair into structured JSON"
  prompt: |
    Extract and format this question-answer pair from a medical textbook.

    QUESTION ID: {question_id}

    QUESTION TEXT (copy verbatim):
    {question_text}

    ANSWER TEXT (copy verbatim):
    {answer_text}

    INSTRUCTIONS:
    1. Copy ALL text EXACTLY as it appears - do not paraphrase or shorten
    2. Extract the question text (before the choices)
    3. Extract each answer choice (A, B, C, D, and E if present)
    4. Copy the full explanation verbatim
    5. If this is a context-only question (no choices), leave choices empty and mark is_context_only: true

    Return ONLY a JSON object:
    {{
      "id": "{question_id}",
      "text": "Full question text copied verbatim",
      "choices": {{
        "A": "Choice A text verbatim",
        "B": "Choice B text verbatim",
        "C": "Choice C text verbatim",
        "D": "Choice D text verbatim"
      }},
      "has_image": true,
      "image_group": "1",
      "correct_answer": "A",
      "explanation": "Full explanation copied verbatim",
      "is_context_only": false
    }}

    For context-only questions (no choices):
    {{
      "id": "{question_id}",
      "text": "Clinical scenario text copied verbatim",
      "choices": {{}},
      "has_image": true,
      "image_group": "1",
      "correct_answer": "",
      "explanation": "",
      "is_context_only": true
    }}

generate_cloze_cards:
  description: "Generate cloze deletion flashcards from explanation text"
  prompt: |
    You are creating Anki cloze deletion flashcards from a medical textbook explanation.

    EXPLANATION TEXT (from {question_id}):
    {explanation}

    TASK: Extract key learning points and create cloze deletion cards.

    CLOZE SYNTAX RULES:
    1. Use {{{{c1::text}}}} for deletions (double braces required in output)
    2. Each card tests ONE key fact or relationship
    3. Related facts can use c1, c2, c3 on the same card
    4. Keep surrounding context for meaning

    GOOD EXAMPLES:
    - "The {{{{c1::foramen spinosum}}}} transmits the {{{{c2::middle meningeal artery}}}}."
    - "{{{{c1::DNET}}}} commonly presents with {{{{c2::intractable seizures}}}} in the {{{{c3::temporal lobe}}}}."
    - "Brainstem gliomas account for {{{{c1::10% to 20%}}}} of all intracranial tumors in children."
    - "{{{{c1::Ganglioglioma}}}} differs from DNET in that {{{{c2::calcification}}}} and {{{{c3::contrast enhancement}}}} are more common."

    BAD EXAMPLES (avoid):
    - "The answer is {{{{c1::C}}}}." (too trivial)
    - "{{{{c1::This}}}} is common." (unclear reference)
    - "The {{{{c1::patient}}}} presented with symptoms." (too generic)

    QUALITY GUIDELINES:
    - Focus on: anatomical structures, pathways, clinical findings, diagnostic criteria, percentages/statistics, differential diagnosis features
    - Extract 2-8 high-quality cards per explanation (more for longer explanations)
    - Skip citations, references, and non-educational filler
    - Each card should stand alone without needing the original question
    - For differential diagnoses, create cards that highlight distinguishing features
    - Include specific numbers, percentages, and measurements when present

    Return ONLY a JSON array (no other text):
    [
      {{
        "cloze_text": "The {{{{c1::foramen spinosum}}}} transmits the {{{{c2::middle meningeal artery}}}}.",
        "learning_point": "Foramen spinosum contents",
        "confidence": "high",
        "category": "anatomy"
      }}
    ]

    Categories to use: anatomy, pathology, imaging, clinical, differential, statistics

    If no suitable content for cloze cards, return: []
